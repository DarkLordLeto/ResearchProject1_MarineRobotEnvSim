{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, Union\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandcoverDiffusionModel:\n",
    "    \"\"\"\n",
    "    A diffusion model for generating realistic land cover maps using a UNet architecture.\n",
    "    The model takes a noise input and gradually denoises it into a coherent land cover map\n",
    "    with different terrain classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_data_path: Union[str, Path] = 'E:/Research/RP1/archive/class_dict.csv'):\n",
    "        \"\"\"\n",
    "        Initialize the land cover diffusion model.\n",
    "        \n",
    "        Args:\n",
    "            class_data_path: Path to CSV file containing class definitions with RGB values\n",
    "        \"\"\"\n",
    "        # Load and validate class definitions\n",
    "        self.class_df = pd.read_csv(class_data_path)\n",
    "        required_columns = {'name', 'r', 'g', 'b'}\n",
    "        if not all(col in self.class_df.columns for col in required_columns):\n",
    "            raise ValueError(f\"Class data CSV must contain columns: {required_columns}\")\n",
    "            \n",
    "        self.num_classes = len(self.class_df)\n",
    "        \n",
    "        # Create color map from CSV data\n",
    "        self.class_colors = {\n",
    "            idx: [r/255.0, g/255.0, b/255.0] \n",
    "            for idx, (_, r, g, b) in enumerate(self.class_df[['r', 'g', 'b']].itertuples())\n",
    "        }\n",
    "        \n",
    "        self.class_names = self.class_df['name'].tolist()\n",
    "        \n",
    "        # Initialize model components\n",
    "        self._initialize_unet()\n",
    "        self._initialize_scheduler()\n",
    "        \n",
    "    def _initialize_unet(self):\n",
    "        \"\"\"Initialize the UNet model with appropriate architecture for land cover generation.\"\"\"\n",
    "        self.unet = UNet2DModel(\n",
    "            sample_size=256,\n",
    "            in_channels=self.num_classes,\n",
    "            out_channels=self.num_classes,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "            down_block_types=(\n",
    "                \"DownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "                \"AttnDownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "            ),\n",
    "            up_block_types=(\n",
    "                \"UpBlock2D\",\n",
    "                \"AttnUpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def _initialize_scheduler(self):\n",
    "        \"\"\"Initialize the noise scheduler for the diffusion process.\"\"\"\n",
    "        self.noise_scheduler = DDPMScheduler(\n",
    "            num_train_timesteps=1000,\n",
    "            beta_start=0.00085,\n",
    "            beta_end=0.012,\n",
    "        )\n",
    "    \n",
    "    def generate_landcover_map(\n",
    "        self, \n",
    "        initial_noise: Optional[torch.Tensor] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        batch_size: int = 1,\n",
    "        image_size: Tuple[int, int] = (256, 256),\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate a land cover map using the diffusion model.\n",
    "        \n",
    "        Args:\n",
    "            initial_noise: Optional initial noise tensor. If None, random noise will be generated\n",
    "            num_inference_steps: Number of denoising steps\n",
    "            batch_size: Number of maps to generate in parallel\n",
    "            image_size: Size of the output map (height, width)\n",
    "            device: Device to run generation on. If None, will use CUDA if available\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, num_classes, height, width) containing class probabilities\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            print(f\"Using CUDA device: {torch.cuda.get_device_name()}\")\n",
    "            \n",
    "        self.unet.to(device)\n",
    "        \n",
    "        if initial_noise is None:\n",
    "            initial_noise = torch.randn(\n",
    "                (batch_size, self.num_classes, *image_size),\n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        self.noise_scheduler.set_timesteps(num_inference_steps)\n",
    "        current_noise = initial_noise\n",
    "        \n",
    "        # Denoising loop\n",
    "        for t in self.noise_scheduler.timesteps:\n",
    "            with torch.no_grad():\n",
    "                noise_pred = self.unet(current_noise, t).sample\n",
    "                \n",
    "            current_noise = self.noise_scheduler.step(\n",
    "                noise_pred,\n",
    "                t,\n",
    "                current_noise\n",
    "            ).prev_sample\n",
    "        \n",
    "        # Get final class probabilities\n",
    "        return F.softmax(current_noise, dim=1)\n",
    "\n",
    "    def visualize_landcover(\n",
    "        self,\n",
    "        generated_map: Union[torch.Tensor, np.ndarray],\n",
    "        save_path: Optional[Union[str, Path]] = None,\n",
    "        figure_size: Tuple[int, int] = (10, 10),\n",
    "        dpi: int = 300\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the generated land cover map with a color-coded image.\n",
    "        \n",
    "        Args:\n",
    "            generated_map: Tensor of shape (batch_size, num_classes, height, width)\n",
    "            save_path: Optional path to save the visualization\n",
    "            figure_size: Size of the output figure in inches\n",
    "            dpi: DPI for saved figure\n",
    "        \"\"\"\n",
    "        # Convert to numpy if needed\n",
    "        if torch.is_tensor(generated_map):\n",
    "            generated_map = generated_map.detach().cpu().numpy()\n",
    "        \n",
    "        class_predictions = np.argmax(generated_map[0], axis=0)\n",
    "        height, width = class_predictions.shape\n",
    "        rgb_image = np.zeros((height, width, 3))\n",
    "        \n",
    "        # Create colored image\n",
    "        for class_idx, color in self.class_colors.items():\n",
    "            mask = class_predictions == class_idx\n",
    "            rgb_image[mask] = color\n",
    "            \n",
    "        plt.figure(figsize=figure_size)\n",
    "        plt.imshow(rgb_image)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [\n",
    "            plt.Rectangle((0, 0), 1, 1, fc=color)\n",
    "            for color in self.class_colors.values()\n",
    "        ]\n",
    "        plt.legend(\n",
    "            legend_elements,\n",
    "            self.class_names,\n",
    "            loc='center left',\n",
    "            bbox_to_anchor=(1, 0.5)\n",
    "        )\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=dpi)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def save_model(self, save_dir: Union[str, Path]) -> None:\n",
    "        \"\"\"\n",
    "        Save the model state and configuration.\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save model files\n",
    "        \"\"\"\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        torch.save(self.unet.state_dict(), save_dir / 'unet.pt')\n",
    "        self.noise_scheduler.save_config(save_dir / 'scheduler_config.json')\n",
    "        self.class_df.to_csv(save_dir / 'class_dict.csv', index=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, load_dir: Union[str, Path]) -> 'LandcoverDiffusionModel':\n",
    "        \"\"\"\n",
    "        Load a saved model from disk.\n",
    "        \n",
    "        Args:\n",
    "            load_dir: Directory containing saved model files\n",
    "            \n",
    "        Returns:\n",
    "            Loaded LandcoverDiffusionModel instance\n",
    "        \"\"\"\n",
    "        load_dir = Path(load_dir)\n",
    "        if not load_dir.exists():\n",
    "            raise ValueError(f\"Model directory {load_dir} does not exist\")\n",
    "            \n",
    "        # Initialize new model with saved class definitions\n",
    "        model = cls(class_data_path=load_dir / 'class_dict.csv')\n",
    "        \n",
    "        # Load saved states\n",
    "        model.unet.load_state_dict(\n",
    "            torch.load(load_dir / 'unet.pt')\n",
    "        )\n",
    "        model.noise_scheduler = DDPMScheduler.from_config(\n",
    "            load_dir / 'scheduler_config.json'\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandcoverDataset(Dataset):\n",
    "    \"\"\"Dataset for loading and preprocessing landcover mask images\"\"\"\n",
    "    def __init__(self, image_dir: str, class_dict_path: str, target_size: tuple = (256, 256)):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Validate input directory\n",
    "        if not self.image_dir.exists():\n",
    "            raise ValueError(f\"Directory not found: {self.image_dir}\")\n",
    "        \n",
    "        # Load and validate class definitions first\n",
    "        try:\n",
    "            self.class_df = pd.read_csv(class_dict_path)\n",
    "            required_columns = {'name', 'r', 'g', 'b'}\n",
    "            if not all(col in self.class_df.columns for col in required_columns):\n",
    "                raise ValueError(f\"Class CSV must contain columns: {required_columns}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading class definitions: {e}\")\n",
    "        \n",
    "        # Create color to class mapping\n",
    "        self.color_to_class = {\n",
    "            (r, g, b): idx \n",
    "            for idx, (r, g, b) in enumerate(\n",
    "                self.class_df[['r', 'g', 'b']].itertuples(index=False)\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.num_classes = len(self.class_df)\n",
    "        \n",
    "        # Find and validate PNG files\n",
    "        self.image_files = []\n",
    "        for file in self.image_dir.glob('*.png'):\n",
    "            try:\n",
    "                # Test open each image\n",
    "                with Image.open(file) as img:\n",
    "                    if img.mode != 'RGB':\n",
    "                        print(f\"Warning: {file.name} is not in RGB mode. Will convert during loading.\")\n",
    "                self.image_files.append(file)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not open {file.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not self.image_files:\n",
    "            raise ValueError(f\"No valid PNG files found in {image_dir}\")\n",
    "        \n",
    "        self.image_files.sort()\n",
    "        print(f\"Successfully loaded {len(self.image_files)} PNG files\")\n",
    "        print(f\"Number of classes: {self.num_classes}\")\n",
    "        print(f\"Target size: {self.target_size}\")\n",
    "        \n",
    "        # Validate first image completely\n",
    "        self._validate_first_image()\n",
    "\n",
    "    def _validate_first_image(self):\n",
    "        \"\"\"Fully validate the first image to catch potential issues early\"\"\"\n",
    "        try:\n",
    "            sample = self[0]\n",
    "            if not isinstance(sample, torch.Tensor):\n",
    "                raise ValueError(\"Dataset output is not a tensor\")\n",
    "            if sample.shape[0] != self.num_classes:\n",
    "                raise ValueError(f\"Expected {self.num_classes} channels, got {sample.shape[0]}\")\n",
    "            print(\"First image validated successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error validating first image: {e}\")\n",
    "            print(\"Full traceback:\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        try:\n",
    "            # Load and convert image\n",
    "            with Image.open(img_path) as image:\n",
    "                if image.mode != 'RGB':\n",
    "                    image = image.convert('RGB')\n",
    "                \n",
    "                # Get original size before resizing\n",
    "                orig_size = image.size\n",
    "                if orig_size != (2448, 2448):\n",
    "                    print(f\"Warning: Image {img_path.name} size is {orig_size}, expected (2448, 2448)\")\n",
    "                \n",
    "                # Resize image\n",
    "                image = image.resize(self.target_size, Image.Resampling.NEAREST)\n",
    "                image_array = np.array(image)\n",
    "\n",
    "            # Create one-hot encoded tensor\n",
    "            target = torch.zeros((self.num_classes, *self.target_size))\n",
    "            \n",
    "            # Track if any pixels are unclassified\n",
    "            classified_pixels = np.zeros(image_array.shape[:2], dtype=bool)\n",
    "            \n",
    "            # Convert RGB to class indices\n",
    "            for color, class_idx in self.color_to_class.items():\n",
    "                mask = np.all(image_array == color, axis=2)\n",
    "                target[class_idx][mask] = 1\n",
    "                classified_pixels |= mask\n",
    "            \n",
    "            # Check for unclassified pixels\n",
    "            unclassified = ~classified_pixels\n",
    "            if np.any(unclassified):\n",
    "                unclassified_colors = set(map(tuple, image_array[unclassified].reshape(-1, 3)))\n",
    "                print(f\"Warning: Image {img_path.name} contains unclassified colors: {unclassified_colors}\")\n",
    "            \n",
    "            return target\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing image {img_path}:\")\n",
    "            print(f\"Error type: {type(e).__name__}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            print(\"Full traceback:\")\n",
    "            traceback.print_exc()\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_landcover_model(\n",
    "    data_dir: str,\n",
    "    class_dict_path: str,\n",
    "    output_dir: str,\n",
    "    num_epochs: int = 10,\n",
    "    batch_size: int = 4,\n",
    "    learning_rate: float = 1e-4,\n",
    "    device: str = None,\n",
    "    save_interval: int = 10\n",
    "):\n",
    "    \"\"\"Training function with enhanced error handling\"\"\"\n",
    "    try:\n",
    "        # Initialize dataset with more detailed error reporting\n",
    "        print(\"Initializing dataset...\")\n",
    "        dataset = LandcoverDataset(data_dir, class_dict_path)\n",
    "        \n",
    "        # Create DataLoader with reduced number of workers and enabled debugging\n",
    "        print(\"Creating DataLoader...\")\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Set to 0 for debugging\n",
    "            pin_memory=False  # Disable pin_memory for debugging\n",
    "        )\n",
    "        \n",
    "        # Test the first batch\n",
    "        print(\"Testing first batch...\")\n",
    "        first_batch = next(iter(dataloader))\n",
    "        print(f\"First batch shape: {first_batch.shape}\")\n",
    "        \n",
    "        # Continue with rest of training...\n",
    "            # Initialize model\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(f\"Using CUDA device: {torch.cuda.get_device_name()}\")\n",
    "            \n",
    "        model = LandcoverDiffusionModel()\n",
    "        model.unet.to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "        optimizer = torch.optim.AdamW(model.unet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "        losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            model.unet.train()\n",
    "            epoch_losses = []\n",
    "        \n",
    "            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "            for batch in progress_bar:\n",
    "                batch = batch.to(device)\n",
    "            \n",
    "            # Sample noise to add to the images\n",
    "                noise = torch.randn_like(batch)\n",
    "                timesteps = torch.randint(\n",
    "                    0, model.noise_scheduler.num_train_timesteps, \n",
    "                    (batch.shape[0],), device=device\n",
    "                ).long()\n",
    "            \n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "                noisy_images = model.noise_scheduler.add_noise(batch, noise, timesteps)\n",
    "            \n",
    "            # Get the model prediction for the noise\n",
    "                noise_pred = model.unet(noisy_images, timesteps).sample\n",
    "            \n",
    "            # Calculate the loss\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            # Update model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                epoch_losses.append(loss.item())\n",
    "                progress_bar.set_postfix({'loss': sum(epoch_losses) / len(epoch_losses)})\n",
    "        \n",
    "        # Save model checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                model.save_model(output_dir + f'checkpoint_epoch_{epoch+1}')\n",
    "        \n",
    "        # Record average epoch loss\n",
    "            avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "            losses.append(avg_loss)\n",
    "        \n",
    "        # Plot and save loss curve\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(losses)\n",
    "            plt.title('Training Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.savefig(output_dir + 'loss_curve.png')\n",
    "            plt.close()\n",
    "        \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\nError during initialization:\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Successfully loaded 803 PNG files\n",
      "Number of classes: 7\n",
      "Target size: (256, 256)\n",
      "First image validated successfully\n",
      "Creating DataLoader...\n",
      "Testing first batch...\n",
      "First batch shape: torch.Size([4, 7, 256, 256])\n",
      "Using CUDA device: NVIDIA GeForce RTX 3070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/201 [00:00<?, ?it/s]d:\\COMPUTER_SCIENCE\\CONDA\\envs\\new_env\\lib\\site-packages\\diffusers\\configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "d:\\COMPUTER_SCIENCE\\CONDA\\envs\\new_env\\lib\\site-packages\\diffusers\\models\\attention_processor.py:3286: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n",
      "Epoch 1/10: 100%|██████████| 201/201 [36:14<00:00, 10.82s/it, loss=0.118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 0.117926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 201/201 [35:21<00:00, 10.56s/it, loss=0.04]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Average Loss: 0.039968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 201/201 [32:34<00:00,  9.72s/it, loss=0.0269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Average Loss: 0.026862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 201/201 [32:21<00:00,  9.66s/it, loss=0.0232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Average Loss: 0.023182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 201/201 [32:31<00:00,  9.71s/it, loss=0.018] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Average Loss: 0.018047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 201/201 [32:10<00:00,  9.61s/it, loss=0.018] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Average Loss: 0.017953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 201/201 [32:31<00:00,  9.71s/it, loss=0.013] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Average Loss: 0.013024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 201/201 [32:23<00:00,  9.67s/it, loss=0.0136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Average Loss: 0.013565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 201/201 [29:45<00:00,  8.88s/it, loss=0.0136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Average Loss: 0.013628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 201/201 [31:36<00:00,  9.44s/it, loss=0.0105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Average Loss: 0.010543\n"
     ]
    }
   ],
   "source": [
    "train_landcover_model(\n",
    "    data_dir='E:/Research/RP1/archive/train/',\n",
    "    class_dict_path='E:/Research/RP1/archive/class_dict.csv',\n",
    "    output_dir='E:/Research/RP1/archive/landcover_model_output/'\n",
    "\n",
    "    #target_size=(512, 512)  # Would need to add this parameter to the function definition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\letol\\AppData\\Local\\Temp\\ipykernel_34032\\432597989.py:205: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(load_dir / 'unet.pt')\n",
      "d:\\COMPUTER_SCIENCE\\CONDA\\envs\\new_env\\lib\\site-packages\\diffusers\\configuration_utils.py:245: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated landcover map saved to E:\\Research\\RP1\\archive\\landcover_model_output\\generated_landcover_map.png\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"E:/Research/RP1/archive/landcover_model_output/checkpoint_epoch_10\"  # Adjust path to your saved model\n",
    "model = LandcoverDiffusionModel.load_model(model_dir)\n",
    "\n",
    "# Generate a sample map\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.unet.to(device)\n",
    "\n",
    "# Generate the landcover map\n",
    "generated_map = model.generate_landcover_map(\n",
    "    batch_size=1,\n",
    "    num_inference_steps=50,  # You can adjust this - more steps = potentially better quality\n",
    "    image_size=(256, 256),  # Match the training size\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize and save the result\n",
    "output_path = Path(\"E:/Research/RP1/archive/landcover_model_output/generated_landcover_map.png\")\n",
    "model.visualize_landcover(\n",
    "    generated_map,\n",
    "    save_path=output_path,\n",
    "    figure_size=(12, 12),\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "print(f\"Generated landcover map saved to {output_path.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
